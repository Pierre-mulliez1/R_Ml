---
title: "Group Assignment Advanced R - COVID19-Global-forecasting-week 4"
output:
  html_notebook: 
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Objectives {#obj}

The objectives of the project are:  

1. Explore, describe and analyze the data by performing an EDA.

2. Report findings from the EDA and give some insights from the different visualizations.

3. Embedding the necessary EDA sections into an interactive shinny app.

# Problem definition {#problem}

For this project we will work with a Kaggle dataset from the "COVID19 Global Forecasting (Week 4)" [link](https://www.kaggle.com/c/covid19-global-forecasting-week-4). As described in kaggle, the objective of the contest is to "forecasting confirmed cases and fatalities between April 15 and May 14 by region." It continues by saying, "the primary goal isn't only to produce accurate forecasts. Itâ€™s also to identify factors that appear to impact the transmission rate of COVID-19." The datasets used for this project is explored below.

# Libraries {#libraries}

To accomplish all objectives we'll be utilizing the following libraries. Information about these libraries and material used in the development of this project please check [annex I](#annex-1) and [annex II](#annex-2), respectively.

```{r libraries, include=FALSE}

# Loading libraries
library(kableExtra)
library(data.table)
library(knitr)
library(papeR)
library(leaflet)
library(corrplot)
library(RColorBrewer)
library(DataExplorer)
library(ggplot2)
library(scales)
library(gtable)
library(grid)
library(egg)
library(gridExtra)
library(factoextra)
library(dplyr)
library(caret)
library(Amelia)
library(tseries)
library(forecast)
library(FactoMineR)
library(urca)

library(tidyr)
library(psych)
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE) # Suppress summarise info
library(plotly)
library(readxl)
library(leaflet.extras)
library(radiant)
library(ggthemes)
```

# EDA {#eda}

## Datasets {#data}

```{r include=FALSE}
# Loading all relevant files
## creating path to folder - easy to change later
folder_path <- "/Users/germandesouza/Desktop/Advanced R EDA/Data"
path_location = "/Users/germandesouza/Desktop/Advanced R EDA/Data/country_location.xlsx"
## assigning to variable
train <- as.data.table(read.csv(file.path(folder_path,"train.csv"))) # created as dt
test <- as.data.table(read.csv(file.path(folder_path,"test.csv")))
coordinates <- read_excel(path_location)
coordinates <- data.frame(coordinates)
```

We have been provided with three different files (training, test and submission data sets), however we feel the need of solely analyzing the training data set. The main reasoning behind this is because the submission is a sample for formatting purposes and the test files are partially replicating the same information seen in the training file, but with different time horizons:

- `train.csv`: This file contains training data with 6 main features (`ForectastId`,`Province_State`,`Country_Region`,`Date`,`ConfirmedCases`,`Fatalities`) between the date ranges of **`r min(as.Date(train$Date))`** and **`r max(as.Date(train$Date))`**.

- `test.csv`: This file contains test data with 4 main features (`ForectastId`,`Province_State`,`Country_Region`,`Date`) between the date ranges of **`r min(as.Date(test$Date))`** and **`r max(as.Date(test$Date))`**. --> **Notice how both training and test have a week of overlap with the training data.**

- `submission.csv`: A sample submission in the correct format; again, predictions should be *cumulative*.

## Understanding the training data set {#understanding}

### Overall structure {#structure}

```{r include=FALSE}
# Checking feature classes to create a table for Rmarkdown
sapply(train,class)
dim(train)
length(unique(train$Country_Region))
dim(train[,uniqueN(Id),.(Date)])
```

Regarding the structure of the COVID-19 week 4 training data set, the data set accounts with **`r dim(train)[1]` observations** and **`r dim(train)[2]` features**. Each observation corresponds to a specific country registry of COVID data (in this case, `ConfirmedCases` and `Fatalities` data) on a daily basis. In this data set we have a total number of **`r length(unique(train$Country_Region))` countries** in a timespan of **`r dim(train[,uniqueN(Id),.(Date)])[1]` days** starting from **`r min(as.Date(train$Date))`** to **`r max(as.Date(train$Date))`**. Below you can find a high-level view of the structure, different data types and the assignation of different feature types that we are going to work with.

| Feature Name | Data Type | Information | Information Feature Type |  |
|-|-|-|-|-|
| **ForecastId** | Integer | Unique ID of forecast | Independent Variable |  |
| **Province_State** | Factorial | Province state of the country | Independent Variable |  |
| **Country_Region** | Factorial | Country name | Independent Variable |  |
| **Date** | Factorial | Upload date of COVID registered data | Independent Variable |  |
| **ConfirmedCases** | Numeric | Confirmed cases of Covid-19 | Target Variable |  |
| **Fatalities** | Numeric | Fatalities caused by Covid-19 | Target Variable |  |

### Checking for data completeness

```{r include=FALSE}
# If we check nulls in all columns from the dataset, we can see that there are no nulls. However, this isn't true as there are blank values in column "Province_State"
cbind(lapply(lapply(train, is.na), sum))

# Checking for blank values in "Province_State"
sapply(train, function(x) sum(is.na(x) | x == "")) # As you can see, Province_State contains lots of empty values

# Checking proportion of blank values within the column "Province_State"
sapply(train, function(x) sum(is.na(x) | x == ""))[2]/dim(train)[1] # Almost a 57,5% of blank values, lets check the countries that have "Province_State"

# Checking which countries don't have blanks in "Province_State"
hello <- distinct(train %>% filter(Province_State!="") %>% transmute(Country_Region))
hello$Country_Region <- as.factor(as.character(hello$Country_Region))

length(unique(train$Country_Region)) # There are 184 countries as we have mentioned in the section above

paste(c("The countries with Province/State informed:", levels(hello$Country_Region)), collapse=" ") #There are 8 Countries that have "Province_State" out of 184 Countries
```

In terms of data completeness, we can state that there aren't null values within the data set, however our team has noticed that there are various blank values in the `Province_State` column. We have identified Almost a **`r round(((sapply(train, function(x) sum(is.na(x) | x == ""))[2]/dim(train)[1])*100),2)`%** of blank values within the training data set. Further implementing data exploration, we were capable of distinguishing countries that had blank values and those that didn't. Interestingly, there are 8 countries that didn't have blank `Province_States`, these are as follows **`r paste(c(levels(hello$Country_Region)), collapse=", ")`**. There are approximately 176 countries without `Province_States`, this is a fundamental point that we will have to take into consideration when we build our models. 

For representation purposes, below we have attached a Missingness Map with Na values. Before executing this map, we have transformed the blank spaces to Na's. By doing this, we can visually see the severity of predicting straight away without some data enhancement to the current training data set.

```{r echo=FALSE, message=FALSE, warning=FALSE}
dat2 <- as.data.table(apply(train, 2, function(x) gsub("^$|^ $", NA, x)))
missmap(dat2, 
        col = c("red", "grey"), 
        x.cex = 0.5,
        y.cex = 0.5,
        legend = FALSE)
```
### Overall COVID-19 Analysis
```{r include=FALSE}
#Data manipulation to create sunburst
sunburst <- train[,c("Country_Region","Province_State","ConfirmedCases","Fatalities")] %>% group_by(Country_Region,Province_State) %>% summarize(TotalConfirmed=sum(ConfirmedCases),TotalFatalities=sum(Fatalities))
```

Now going deeper into the numbers, we can extract the following insights:

- From **`r min(as.Date(train$Date))`** to **`r max(as.Date(train$Date))`**, the total cummulative number of confirmed COVID-19 cases worldwide reached to **`r formatC(sum(sunburst$TotalConfirmed), format="d", big.mark=",")`** and **`r formatC(sum(sunburst$TotalFatalities), format="d", big.mark=",")`** total cummulative fatalities. A proportion of **`r round(sum(sunburst$TotalFatalities)/sum(sunburst$TotalConfirmed)*100,2)`%** from worldwide confirmed cases. As it is seen in the figure "Worldwide COVID-19 Evolution (January-May)" the tangent of total confirmed cases per day is way higher than the ones seen in total fatalities per day.

```{r, out.width="100%"}
comp <- train %>% group_by(Date) %>% summarize(TotalConfirmed=sum(ConfirmedCases),TotalFatalities=sum(Fatalities))

plot1 <- plot_ly(comp, x = ~Date, showlegend = T, width = 1000) %>%
  add_lines(y = ~TotalConfirmed, name="Total Confirmed" ,line = list(color = "red"))%>%
  add_lines(y = ~TotalFatalities, name="Total Fatalities", line = list(color = "green"))%>%
  layout(title = 'Worldwide COVID-19 Evolution (January-May)', 
         yaxis=list(title="Number of People"),
         xaxis=list(title="Date"))
plot1
```
- As it can be represented in the bar plot below, **USA is seen as the country with the most confirmed cases worldwide**. The other two consecutive countries that follows the USA is **Italy and Spain**. It would be interesting to analyse the evolution of these three countries in terms of total confirmed cases and fatalities over time. 

```{r, out.width="100%"}
comp <- train %>% group_by(Country_Region) %>% summarize(TotalConfirmed=sum(ConfirmedCases),TotalFatalities=sum(Fatalities)) %>% arrange(desc(TotalConfirmed), .by_group = TRUE) %>% top_n(10,TotalConfirmed)

comp[,1] <- lapply(comp[,1], as.character)

plot2 <- plot_ly(comp, x=comp$Country_Region,y = comp$TotalConfirmed, showlegend = T, width = 1000, type="bar", name="Total Confirmed")%>%
  add_trace(y=comp$TotalFatalities, name= "Total Fatalities")%>%
  layout(title = 'Top 10 Countries with the most COVID-19 Confirmed cases & Fatalities (January-May)',
         xaxis = list(autorange = "reversed",title="Country"),
         yaxis=list(title="Number of People"))

plot2
```

- As we can see in the comparative barplot below, Italy has been the country that has been affected the earliest out of the top 3 countries with most confirmed cases. However, Italy's curve was the slowest compared to the other 2 countries. Spain was the second fastest country to increase its cumulative confirmed cases throughout time. Nevertheless, the US has rapidly scaled in terms of COVID-19 cases. **It would be interesting to do a deeper analysis per province state in the US. This could help us to determine which provinces have experienced a greater impact of the pandemic**. However due to the large number of provinces we will stick with the 5 most relevant ones.

```{r, out.width="100%"}
comp <- train %>% group_by(Date,Country_Region) %>% summarize(TotalConfirmed=sum(ConfirmedCases),TotalFatalities=sum(Fatalities)) %>% filter(Country_Region %in% c("US","Spain","Italy"))

top3c <- function(method = "US") {
  comp <- comp %>% filter(Country_Region %in% method)
  plot_ly(x = comp$Date, y = comp$TotalConfirmed, name= method, type="bar", width = 1000)}

subplot(
  top3c(), top3c("Spain"), top3c("Italy"),
  nrows = 3, shareX = TRUE
)%>%
  layout(title = 'Comparative between the top 3 countries with most confirmed cases (January-May)',
         xaxis = list(title="Date"),
         yaxis=list(title="Confirmed cases"))
```

- As we can see in the scatter plot below, **New York has been the province mostly affected by the exponential rise of confirmed cases in the US**. It shows an exploding incremental progression of confirmed cases, outpacing other precedent provinces in the US. Second it would be the **New Jersey** and the other three provinces are at par in terms of evolution.

```{r, out.width="100%", warning=FALSE}
comp <- (train %>% group_by(Country_Region,Province_State) %>% summarize(TotalConfirmed=sum(ConfirmedCases),TotalFatalities=sum(Fatalities)) %>% filter(Country_Region %in% c("US"))%>% arrange(desc(TotalConfirmed), .by_group = TRUE) %>% top_n(5,TotalConfirmed))[,2]

comp[,1] <- lapply(comp[,1], as.character)
comp <- as.vector(comp)

comp <- train %>% group_by(Date,Country_Region,Province_State) %>% summarize(TotalConfirmed=sum(ConfirmedCases),TotalFatalities=sum(Fatalities)) %>% filter(Country_Region %in% c("US"))%>% filter(Province_State %in% c("New York","New Jersey","Massachusetts","Illinois","California"))

plot_ly(comp, x = ~Date, y = ~TotalConfirmed, color = ~Province_State, mode="markers" ,type = "scatter", width = 1000)%>%
  layout(title = 'Top 5 provinces in USA with the highest levels of confirmed cases (January-May)',
         xaxis = list(title="Date"),
         yaxis=list(title="Confirmed cases"))
```


### Univariate Analysis per Country

In this section our team aims to learn the overall distribution and dispersion of target variables (`Fatalities`,`ConfirmedCases`). In the first two tables below we will show a summary statistics table highlighting relevant metrics per country. 

#### Confirmed COVID-19 Cases

As we have explained in the "Overall COVID 19 Analysis", the top countries with the maximum number of cases are the US, Italy and Spain. However, it is very interesting to observe a large discrepancy in terms of value ranges, median's and standard deviations. Therefore, our team attaches this descriptive table for further EDA.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
hello2 <- train[,3:5] 
hello2 <- hello2 %>% group_by(Date, Country_Region) %>% summarise(ConfirmedCases = sum(ConfirmedCases))
hello2 <- pivot_wider(hello2, names_from = Country_Region, values_from = ConfirmedCases)

confirmed <- as.data.frame(describe(hello2[,2:185])) # descriptive statistics using psych
confirmed <- confirmed[order(-confirmed$mean),3:13]
confirmed
```

#### Fatalities COVID-19 Cases

Regarding fatalities, we can observe that there are new countries in at the top of the list compared to the table above. Countries such as **Belgium and Brasil** although they weren't able to make it on the top 10 list in terms of confirmed cases, they managed to be one of the countries with highest mortality rates from COVID-19. This is interesting to assess underlying factors triggering these results (for instance: population, government expenditure on healthcare, healthcare data).

```{r, echo=FALSE, message=FALSE, warning=FALSE}
hello3 <- train[,3:4] %>% mutate(Fatalities=train$Fatalities)
hello3 <- hello3 %>% group_by(Date, Country_Region) %>% summarise(Fatalities = sum(Fatalities))
hello3 <- pivot_wider(hello3, names_from = Country_Region, values_from = Fatalities)

fatalities <- as.data.frame(describe(hello3[,2:185]))# descriptive statistics using psych
fatalities <- fatalities[order(-fatalities$mean),3:13]
fatalities
```

### Visualising Geospatial Data COVID-19

This visualisation aids us to determine if there is a geolocation pattern in terms of the COVID spread. COVID spreadability has been scattered all around the globe, it is very difficult to determine a pattern as it is dependent on other factors as we have stated in section "Fatalities COVID-19 Cases".

```{r, warning=FALSE, out.width="100%"}
new_training <- train %>% group_by(Country_Region)%>% summarise(Fatalities = sum(Fatalities),Cases = sum(ConfirmedCases))
new_training <- merge(x = new_training,y = coordinates,by.x = "Country_Region", by.y = "name",all = FALSE)

#p <- new_training[,7:8]
#setDT(p)[, names(p) := lapply(.SD, function(x) if(is.factor(x)) as.numeric(as.character(x)) else x)]

#new_training[,7:8] <- p

new_training$size <- 10000
new_training[new_training$Cases > 5000,]$size <-  80000
new_training[new_training$Cases > 10000,]$size <-  150000

new_training$col <- "blue"
new_training[new_training$Fatalities > 2000,]$col <-  "darkblue"
new_training[new_training$Fatalities > 10000,]$col <-  "red"

new_training$countryC <- paste("country", ", Cases: " ,toString(0))
for (country in new_training$Country_Region){
  (new_training[new_training$Country_Region == country,]$countryC
   <- paste(country, ", Cases: " ,formatC((new_training[new_training$Country_Region == country,]$Cases), format="d", big.mark=",")))
}

cov_map <- (leaflet(data = new_training,width = "100%") %>% 
              addProviderTiles(providers$Stamen.TonerBackground) %>% 
              addCircles(lng = new_training$longitude, lat = new_training$latitude, popup = ~countryC, color = ~col, radius = ~size,fill = F))%>% 
              setView(11, 49,  zoom = 2) %>%
  addMiniMap(zoomLevelOffset = -4) %>%
            addResetMapButton()

cov_map <-  addLegend(cov_map,"bottomright", 
                  labels = c("<2000","2000+","10000+"), 
                  colors = c("blue","darkblue","red"),
                  title = "Fatalities",
                  opacity = 1) 
cov_map
```

### Weekday Seasonality

In order to get a better idea if there is an existing pattern in the different days of the week in distinct months, we wanted to give a visualisation alternative (heatmap) to identify if there was a pattern in terms of weekday seasonality. As we can see, the first months there is no conclusive pattern, however as we approach to March we start to see some patterns on indicative days. But still, this is not enough to drive conclusive insights given the short limited scope per country and the limited amount of data provided throughout different months.

```{r, warning=FALSE, out.width="100%"}
train %>% 
  mutate(wday = wday(Date, label = TRUE),
         month = month(Date, label = TRUE),
         year = year(Date)) %>% 
  group_by(wday, month, year) %>% 
  summarise(cases = sum(ConfirmedCases)/1e6) %>%
  ggplot(aes(month, wday, fill = cases)) +
  geom_tile() +
  labs(x = "Month of the year", y = "Day of the week", fill = "Relative Confirmed Cases") +
  scale_fill_distiller(palette = "Spectral") +
  theme_hc()
```

# Annex I {#annex-1}

This section list libraries we used in the realization of this project.


## For data manipulation

[data.table](http://127.0.0.1:32237/help/library/data.table/doc/datatable-intro.html)


[dplyr](https://dplyr.tidyverse.org)  
Provides a consistent set of verbs (mutate, select, filter, ...) that help you solve the most common data manipulation challenges.


[Amelia](https://cran.r-project.org/web/packages/Amelia/index.html)  
A program for missing data

## For reporting

[knitr](https://yihui.org/knitr/)  
Elegant, flexible, and fast dynamic report generation with R


## For tables

kable and kableExtra ([link](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html#Table_Styles))  
We used these libraries to create awesome HTML tables.

## For data exploration

[DataExplorer](https://cran.r-project.org/web/packages/DataExplorer/index.html)  
This library supports in the scan, analysis and visualization of variables.

[dlookr](https://cran.r-project.org/web/packages/dlookr/index.html)  
Supports data diagnosis, exploration, and transformation.


## For mapping

[leaflet](https://rstudio.github.io/leaflet/)  
Provides support in the creation of interactive maps.


## For statistical reporting

[papeR](https://cran.r-project.org/web/packages/papeR/vignettes/papeR_introduction.html)  
We used this library to handle variable labels and to create (complex) summary tables. 


## For plots

[ggplot2](https://ggplot2.tidyverse.org)  
For graphics creation


[scales](https://www.rdocumentation.org/packages/scales/versions/0.4.1)
Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.


gtable, grid, egg, gridExtra ([link](https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html))  
Used to layout multiple plots in a page. See explanation in link.


[corrplot](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)  
To display a graphical correlation matrix (correlation plot) for fast data understanding.


[RColorBrewer](https://www.rdocumentation.org/packages/RColorBrewer/versions/1.1-2/topics/RColorBrewer)  
Provides nice looking color palettes


[Factoextra](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization)  
Visualization of multivariate data analysis results


## For prediction

[caret](https://topepo.github.io/caret/)  
Streamsline the process for creating predictive models


[forecast](https://cran.r-project.org/web/packages/forecast/forecast.pdf)  
Forecasting functions for time series and linear models


## For time series

[tseries](https://cran.r-project.org/web/packages/tseries/index.html)  
Time series analysis

# Annex II {#annex-2}

This section list useful links used in the realization of this project.

For EDA: 
[data explorer](https://rpubs.com/mark_sch7/DataExplorerPackage)

For plotting: 
[ablines](http://www.sthda.com/english/wiki/abline-r-function-an-easy-way-to-add-straight-lines-to-a-plot-using-r-software), 
[ggplot](https://datacarpentry.org/R-ecology-lesson/04-visualization-ggplot2.html), 
[ggplot2](https://lgatto.github.io/2017_11_09_Rcourse_Jena/data-visualization-with-ggplot2.html), 
[ggplot3](https://www.datanovia.com/en/blog/ggplot-axis-labels/)

For RMarkdown: 
[cross-referencing](https://ulyngs.github.io/oxforddown/cites-and-refs.html#cross-referencing), 
[notebook](https://bookdown.org/yihui/rmarkdown/notebook.html), 
[notebook2](http://uc-r.github.io/r_notebook)

For time series: 
[fpp2](https://cran.r-project.org/web/packages/fpp2/index.html)

For machine learning: 
[mlbench](https://www.rdocumentation.org/packages/mlbench/versions/2.1-1)

For dimentionality reduction: 
[PCA](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/)

For solving issues in respect to compilers: 
[solution for installing Amelia](https://github.com/immunogenomics/harmony/issues/113)

For understanding the state of r packages for data exploration
[article](https://arxiv.org/pdf/1904.02101.pdf)